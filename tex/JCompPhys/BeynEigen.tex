\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{fontenc,multicol,amsmath,amssymb,comment,graphicx}
\usepackage[sort&compress,numbers]{natbib}
\usepackage{subfig,siunitx,setspace,xcolor}
\usepackage{algorithm, algorithmic}

\addtolength{\hoffset}{-2cm}
\addtolength{\textwidth}{4cm}
\addtolength{\voffset}{-2.5cm}
\addtolength{\textheight}{4cm}
%----------commands----------------------------------%
\newcommand{\diff}{\mathrm{d}}
%------------------------------------------------------%
%\pagestyle{myheadings}
\pagestyle{plain}
%------------------------------------------------------%
\title{Method for Nonlinear Eigenproblems: Hybridizing Contour Integral and Newton's Method} 
\author{Yoonkyung Eunnie Lee}
\date{Last updated: \today}
%------------------------------------------------------%
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
We propose a solution method for nonlinear eigenproblems (NEPs) by hybridizing two known solution methods for NEPs. The first method is based on Newton iteration, to search for the roots of the characteristic equation. The second method is referred to as Beyn's method in this work and is based on contour integration. (CI) Beyn's method is based on the observation that the singular values of an analytic function are equivalent to the poles of the inverse function. The eigenvalues are obtained by performing a CI of the inverse function on the complex plane.  
While Newton's method has excellent quadratic convergence, it requires a good initial guess for each eigenvalue in order to find all of the solutions. On the other hand, Beyn's method requires no initial guess at the cost of having to compute the matrix inverse at every quadrature point, which is computationally expensive to achieve the same convergence. Our hybrid implementation combines the strengths of the two methods. We demonstrate this strength through a polynomial eigenproblem in this report, and provide guidelines for hybridization. We further aim to generalize the solution method towards eigenvalue problems in science and engineering. 

\end{abstract}
Nonlinear Eigenvalue Problems, (NEPs) Contour Integral (CI) Method, Newton Method, Nonlinear Inverse Iteration
%--------------------------------------------------------------
\section{Introduction} 
%--------------------------------------------------------------
Eigenvalue problems (EPs) arise in diverse applications in science and technology, or any other field where a matrix or tensor represents the behavior of a system. \citep{guillaume_nonlinear_1999, betcke2013nlevp}. Solving an EP is equivalent to finding the condition at which the system is singular and therefore can produce a divergent output in response to a finite input. This is closely related to the concept of resonance for physical systems characterized with an eigenfrequency $\omega$. Nonlinearity is extremely common when solving for eigen-frequencies, because a system that contains energy dissipation, gain, memory, or feedback is nonlinear in $\omega$. Nonlinearity can also arise from the boundary conditions or from the use of special basis functions, even when the original system is linear. A nonlinear eigenproblem (NEP) is a generalization of an ordinary EP to equations that depend nonlinearly on the eigenvalue. Solutions for NEPs is a very active research field, filled with real-world applications. In this report, we focus on eigenproblems where nonlinearties arise only from the eigenvalue, not from the eigenvector.  

\subsection{General Framework}
Given an analytic, matrix-valued function $A(\omega)$ defined on some some subdomain $\mathcal{D}$ of the complex plane, we consider the solution of:
\begin{equation}\label{eq:Eig}
    A(\omega) \mathbf{v} = 0,\;\;\;\;\;\mathbf{v}\neq 0
\end{equation}
where $\omega \in \mathcal{D}$ is the nonlinear eigenvalue, and $\mathbf{v} \in \mathbb{C}^n\backslash\{0\}$ is the corresponding eigenvector. Any pair $(\omega, \mathbf{v})$ satisfying \ref{eq:Eig} is called an eigenpair of $A$, and the set of all eigenvalues is commonly known as the spectrum of $A$. For regular NEPs with non-empty set of $\omega$, The mapping $\omega \mapsto A(\omega)^{-1}$ is called the resolvent of $A$ and is well defined for all $\omega$ in the resolvent set. 

\pagebreak
An eigenvalue problem is essentially a root-finding problem because the goal is to search for all of the singular points of an operator. The characteristic equation for an NEP can be written as:
\begin{equation}\label{eq:charNEP}
\det A(\omega) = 0. 
\end{equation}

A linear eigenvalue problem is a special case of an NEP with the choice of 
\begin{equation} 
\label{eq:LEP}
A(\omega )= \omega I - B,
\end{equation}

in \ref{eq:Eig}, where $B \in\ \mathbb{C}^{n \times n}$ is a constant matrix. It is well known that $B$ has at most $n$ distinct eigenvalues that are the roots of the characteristic polynomial

\begin{equation}
	\label{eq:charEP}
	p_{B}(\omega)=\det(\omega I -B)
\end{equation}  

associated with $B$. 

The solution method for linear eigenvalue problems is a mature topic in numerical analysis, and computational libraries are available with implementations of different solution methods including Arnoldi$/$Lanczos with explicit restart, Jacobi-davidson, Rayleigh-quotient conjugate gradient, and Krylov-Schur.  \citep{hernandez2005slepc} 

For linear eigenvalue problems, the eigenvectors can be chosen to be linearly independent from each other. This is essential for reliable computation because the per-determined eigenvectors can be deflated from the problem by restricting the computation to their orthogonal complement. 

On the other hand, the eigenvectors of NEPs are not guaranteed to be linearly independent. This is problematic when solving for multiple eigenpairs together, because the problem cannot be reduced due to the risk of missing other eigenvalues. The number of eigenvalues are also unknown for a NEP. While the characteristic equation for a linear EP in \ref{eq:charEP} is always a polynomial, $\det A$ in \ref{eq:charNEP} is not necessarily a polynomial and therefore the number of eigenvalues are not limited by the size of $A$. For example, the function
\begin{equation*}
A(\omega) = \sin(\omega)
\end{equation*} 
has an infinite number of eigenvalues. Such complications make NEPs harder to solve than linear EPs. 

\subsection{Overview}
In this report, we review existing solution methods for NEPs and suggest to combine CI-based Beyn's method and Newton's iteration to achieve faster convergence to all eigenvalues inside a given contour. We discuss the basic aspects of available solution methods for NEPs in section 2, and review the algorithms for the two methods of interest. In section 3 we suggest and demonstrate the hybrid method. In section 4 we perform error analysis of an example test case using MATLAB's {\tt polyeig} function. Lastly we summarize our results and conclude with a discussion on what future steps to take in order to make this approach useful for research. 

%--------------------------------------------------------------
\section{Solutions of NEPs by Newton's Method }
%--------------------------------------------------------------
We begin by summarizing the existing numerical algorithms for NEPs. It is natural to begin by algorithms based on Newton's root-finding method, since solving an NEP is equivalent to finding all roots of the characteristic equation \ref{eq:charNEP}. Newton's method finds the root of $f(\omega)=0$ by the following iteration:
\begin{equation}
	\label{eq:NewtIterate}
	\omega \rightarrow \omega - \frac{f(\omega)}{f'(\omega)}.
\end{equation}
Below we introduce algorithms that are useful when information about the eigenvector is also wanted.
\subsection{Algorithms based on Newton's Method}
We describe three algorithms based on Newton's method. 
The first is Newton-Rhapson root-searching method for the characteristic equation: 
\begin{equation}\label{eq:NewtonRhapson}
\frac{f(\omega)}{f'(\omega)} = \frac{\mathrm{det}(A(\omega))}{ \mathrm{tr} \left( \mathrm{adj}(A(\omega))\dot{A}(\omega)\right)} = \frac{1}{ \mathrm{tr}\left( A^{-1}(\omega)\dot{A}(\omega)\right)},
\end{equation}
where $A(\omega)$ is an analytic matrix-valued function, and $\dot{A}(\omega)=\diff A(\omega)/\diff \omega$ is the derivative of $A$ in $omega$. We use \ref{eq:NewtonRhapson} to iterate for the eigenvalue search performed in this report. 
\begin{algorithm}
\caption{Newton Rhapson method for $\mathrm{det}(A(\omega))=0$}
\label{alg:NewtonRhapson}
\begin{algorithmic}
\FOR{j=1, 2, \ldots until convergence }
\STATE $\omega_{j+1} = \omega_j - \left\lbrace \mathrm{tr}\left( A^{-1}(\omega)\dot{A}(\omega)\right)\right\rbrace^{-1} $
\ENDFOR
\end{algorithmic}
\end{algorithm}
Algorithm \ref{alg:NewtonRhapson} is implemented in {\tt NewtInv.m}. 
The next method is {\it nonlinear inverse iteration}, \citep{anselone_solution_1968} which takes an initial guess for both the eigenvalue $\omega$ and eigenvector $\mathbf{v}$, with the advantage of reducing the computation cost. 
\begin{algorithm}
\caption{nonlinear inverse iteration}
\label{alg:Newton}
\begin{algorithmic}
\STATE Let $e$ be a normalization vector. Start with an initial guess ($\omega_0, \mathbf{v}_0$) such that $e^{H}\mathbf{v}_0=1$ 
\FOR{j=1, 2, \ldots until convergence } 
\STATE $\mathbf{x}_{j+1} = A(\omega_j)^{-1} \dot{A}(\omega_j)\mathbf{v}_j$
\STATE $\omega_{j+1} = \omega_j - e^H\mathbf{v}_j/e^H\mathbf{x}_{j+1} $
\STATE normalize $\mathbf{v}_{j+1}=\mathbf{x}_{j+1}/e^{H}\mathbf{x}_{j+1}$
\ENDFOR
\end{algorithmic}
\end{algorithm}
Algorithm \ref{alg:Newton} is implemented in {\tt NewtInv2.m}. 

In another variant of Newton's method, called {\it residual inverse iteration}, \citep{neumaier1985residual} $A(\omega_j)$ and $A(\omega_{j+1})$ are both stored to update the residual, with the advantage that $\dot{A}(\omega)$ is no longer needed. This can be advantageous for complicated functions for which the inverse is not known, such as matrix outputs of Boundary Element Method. (BEM)

\begin{algorithm}
\caption{residual inverse iteration}
\label{alg:Newton_resinverse}
\begin{algorithmic}
\STATE Let $e$ be a normalization vector. Start with an initial guess ($\omega_0, \omega_1, \mathbf{v}_0$) such that $e^{H}\mathbf{v}_0=1$ 
\FOR{j=1, 2, \ldots until convergence }
\STATE $\mathbf{v}_{j+1} = \mathbf{v}_j- A(\omega_j)^{-1} A(\omega_{j+1})\mathbf{v}_j $
\STATE $\omega_{j+1} = \omega_j - e^H\mathbf{v}_j/e^H\mathbf{v}_{j+1} $
\ENDFOR
\end{algorithmic}
\end{algorithm}
$A(\omega_j)^{-1}$ in the above algorithm can be further substituted with $A(\omega_0)^{-1}$ without destroying the convergence. The residual inverse iteration is not implemented in this report. 

%--------------------------------------------------------------
\subsection{Operation Count and Convergence}
%--------------------------------------------------------------
Algorithms \ref{alg:Newton} and \ref{alg:Newton_resinverse} both 
have asymptotically quadratic convergence towards simple eigenvalues, meaning that the sequence converges with order $q=2$ to $L$ where 
\begin{equation}\label{eq:quadconv}
    \lim_{k \to \infty} \frac{|x_{k+1}-L|}{|x_k-L|^{2}} = \mu 
\end{equation}
where there exists a number $\mu>0$. Each step of nonlinear inverse iteration of algorithm \ref{alg:Newton} requires a solution of a linear system for $A(\omega)^{-1}$, which asymptotically requires $\mathcal{O}\{m^3\}$ flops. 

\subsection{Other Methods}
Solution methods for NEPs is an active research field, and contributions to the topic is not necessarily limited to Newton-based or CI-based methods. A good review can be found in \citep{effenberger_robust_2013}. 
\subsubsection{Subspace Iteration with Newton-based Methods}
A crucial point in iterative projection methods for general NEPs is to  prohibit repeated convergence to  the same eigenvalue. For this purpose, it can be advantageous to retain the previous approximations when memory is not the biggest concern. A Rayleigh-Ritz procedure for NEPs includes the nonlinear Arnoldi and \citep{voss2004arnoldi} and nonlinear Jacobi-davidson \citep{mehrmann2004nonlinear, voss2007jacobi} method. 
\subsubsection{Block Algorithms for NEPs}
Another interesting approach is to create block versions of aforementioned algorithms, which prevents re-convergence to the same eigenvalue by computing all eigenvalues in a cluster simultaneously, which is similar to subspace iteration for linear eigenvalue problems. Kressner proposed a block Newton method for nonlinear inverse iteration in 2009 \citep{kressner2009block}. 
%--------------------------------------------------------------
\section{Solutions of NEPs by Contour Integral Methods}
%--------------------------------------------------------------
On the other hand, a new class of solution methods for NEPs based on CIs have been developed in the 2000's. It is based on the observation that if we assume all eigenvalues to be distinct, the zeros of an analytic matrix function $A(\omega)$ are equal to the poles of the resolvent $A^{-1}(\omega)$. 

CI-based method for generalized EPs was suggested by Sakurai and Sugiura in 2003, \citep{sakurai2003projection} and was extended for nonlinear eigenproblems using block-Hankel matrices by Asakura {\it et al.} in 2009. \citep{asakura_numerical_2009} The eigenspace is first constructed by CI, and a subspace iteration is used to solve the GEP. (Hankel: \citep{asakura_numerical_2009}, Rayleigh-Ritz: \citep{yokota_projection_2013}) In this report we follow the formulation of Beyn\citep{beyn_integral_2012}, which shares the same root with \citep{asakura_numerical_2009, sakurai_efficient_2013}, but without subspace iteration. 

%--------------------------------------------------------------
\subsection{Theoretical Framework}
%--------------------------------------------------------------
The resolvent $A(\omega)^{-1}$ is a finitely meromorphic function, meaning that there exist $\kappa\in\mathbb{N}$ and $S_j\in \mathbb{C}^{n,n}$ for $j\geq -\kappa$ such that $S_{-\kappa}\neq 0$, and 
\begin{equation}\label{eq:Ainv}
A^{-1}(\omega)=\sum\limits_{j=-\kappa}^{\infty} S_j (\omega-\omega_0)^j,\;\;
\;\; \omega \in \mathcal{U} \setminus \lbrace\omega_0 \rbrace 
\end{equation}
for some neighborhood $\mathcal{U}$ of $\omega_0$. Let us consider a Laurent series expansion of $A(\omega)^{-1}$. According to Keldysh theorem, \citep{keldysh1951characteristic, beyn_integral_2012} the principal part of this Laurent series can be expressed in terms of the (generalized) left and right eigenvectors associated with $\omega$:
\begin{equation}
\label{eq:Keldysh}
A(\omega)^{-1} = \sum\limits_{k}\frac{1}{\omega-\omega_k}\mathbf{v}_k\mathbf{w}_k^H + R(\omega).
\end{equation}
Here $k$ is the number of eigenvalues inside the contour $\Gamma$, $\omega_k$ are the complex eigenvalues, $\mathbf{v}_k, \mathbf{w}_k$ are the corresponding left and right eigenvectors, and $R(\omega)$ is the residual that is holomorphic. Using the residue theorem, we can write: 
\begin{equation}\label{eq:cont}
A_{p} = \frac{1}{2\pi i}\int_{\Gamma}\omega^p A^{-1}(\omega)\hat{M} \diff\omega \;\in\mathbb{C}^{n \times l},
\end{equation}
where $\hat{M} \in \mathbb{C}^{n,l}$ is chosen as a random matrix that preserves the rank $k$ with $k \leq l \leq n$. We have reduced the NEP to a k-dimensional linear EP. The choice of $p=0,1$ is sufficient when $k<n$, that is, if the number of eigenvalues inside the contour is less than the problem dimension. If not, a more general formalism using $p > 1$ should be used. \citep{beyn_integral_2012}

%--------------------------------------------------------------
\subsection{Quadrature Evaluation}
%--------------------------------------------------------------
The contour integrals in eq. \ref{eq:cont} are calculated approximately the by trapezoidal sum of a smooth analytical contour. This leads to an exponential decline of the quadrature error with the number of quadrature nodes.

We implement the contour integral using a circular parametrization $\varphi(t) = g_0 + e^{i t}$, discretized using $t_k = 2\pi k/N$, where $k=0,1,\ldots,(N-1)$. 

\begin{equation}\label{eq:A0int}
A_{0,N} = \frac{1}{2\pi i}\int\limits_{0}^{2\pi} A^{-1}(\varphi(t))\hat{M} \varphi^{\prime}(t)\diff t 
\approx \frac{1}{iN}\sum\limits_{k=0}^{N-1}A(\varphi(t_k))^{-1}\hat{M}\varphi^{\prime}(t_k)
\end{equation}
\begin{equation}\label{eq:A1int}
A_{1,N} = \frac{1}{2\pi i}\int\limits_{0}^{2\pi} A^{-1}(\varphi(t))\hat{M} \varphi(t) \varphi^{\prime}(t)\diff t
\approx \frac{1}{iN}\sum\limits_{k=0}^{N-1}A(\varphi(t_k))^{-1}\hat{M} \varphi(t_k) \varphi^{\prime}(t_k)
\end{equation}

%--------------------------------------------------------------
\subsection{Algorithm using SVD}
%--------------------------------------------------------------
Using the theoretical framework layed out above, the next step is to perform the SVD and solve the linearized eigenproblem. This is summarized in algorithm \ref{alg:Beyn1}. 

\begin{algorithm}
\caption{Beyn's algorithm for a few eigenvalues}
\label{alg:Beyn1}
\begin{algorithmic}
\REQUIRE Choose an index $l\leq n$ and a random matrix $\hat{M}\in\mathbb{C}^{n\times l}$. 
\STATE Compute $A_{0,N}$ and $A_{1,N}$ from \ref{eq:A0int}, \ref{eq:A1int}. 
\STATE Compute the SVD $A_{0,N} = V\Sigma W^{H}$ 
\\ \hspace{0.5cm} where $V\in\mathbb{C}^{n\times l}$, $W\in\mathbb{C}^{l\times l}$, $V^{H}V=W^{H}W = I_{l}$, $\Sigma = \mathrm{diag}(\sigma_1,\sigma_2,\ldots,\sigma_l)$.
\STATE Perform a rank test to find $k$ 
\\ \hspace{0.5cm} by $\sigma_k \geq \mathrm{tol_{rank}}\geq \sigma_{k+1} \approx \ldots \approx \sigma_l \approx 0$. 
\\ \hspace{0.5cm} If $k = l$ then increase $l$ and go to step 1. 
\STATE Compute $B = V_0^H A_{1,N} W_0 \Sigma_0^{-1} \in \mathbb{C}^{k,k}$.
\STATE Solve $B\mathbf{v} =\omega\mathbf{v}$.
\end{algorithmic}
\end{algorithm}
This algorithm is implemented in {\tt Beyn1.m}. 

%--------------------------------------------------------------
\subsection{Operation Count and Convergence}
%--------------------------------------------------------------
CI-based algorithm guarantees that all distinct eigenvalues inside the contour will be located with a choice of a sufficiently large $N$. From the trapezoidal sum of \ref{eq:cont}, the numerical effort for $N$-point Beyn integration is dominated by performing $N$ LU-decompositions and solving $Nk$ linear systems:
\begin{equation}
\mathcal{O}(N m^3) + \mathcal{O}(N k m^3)\approx \mathcal{O}(Nkm^3).
\end{equation}
Therefore, increasing $N$ for convergence is extremely expensive. This convergence will be tested in section 4 in further detail. 
%--------------------------------------------------------------
\section{Numerical Implementation and Error Analysis}
%--------------------------------------------------------------
We implement a solution method for NEPs that are nonlinear in the eigenvalue parameter, by hybridizing Beyn's CI method and Newton's nonlinear inverse iteration method. 


%--------------------------------------------------------------
\section{Strategy for Hybridization}
%--------------------------------------------------------------
First, Beyn's method (algorithm \ref{alg:Beyn1}) is used to solve for a list of approximate eigenvalues. Next we switch to Newton-based iteration to refine the convergence without further increasing the number of Beyn quadrature points $N$. 

The biggest computational cost for Beyn's method is incurred by the increase of $N$, since the operation cost scales with $\mathcal{O}(Nkm^3)$. Newton's method is computationally less expensive but depends on the quality of the initial guess provided. The goal is to ensure convergence of all eigenvalues with the smallest possible $N$. We test this condition with the following numerical example. 
%--------------------------------------------------------------
\subsection{Numerical Example: A Polynomial Eigenproblem}
%--------------------------------------------------------------
\begin{figure}
\label{fig:E}
\begin{center}
\includegraphics[width=6.45cm]{../pictures/poly2_100_E.eps}
\includegraphics[width=6.45cm]{../pictures/poly2_100_E_close.eps}
\end{center}
\caption{Spectrum of $A(\omega)$. \textnormal{Left: spectrum including all eigenvalues of $A$. Right: A closer view around the contour $g_0+\rho e^{it}$ with $g_0=0$, $\rho=0.5$, $N=150$.}}
\end{figure}

Let us consider the polynomial eigenvalue problem:
\begin{equation}
\label{eq:polyeig}
A(\omega)= A_0+A_1 \omega + A_2 \omega^2 + \ldots =\sum\limits_{i=0}^{p}\omega^iA_i ,
\end{equation}
which can be solved exactly using the MATLAB function {\tt polyeig} for the exact solutions. We implement the Beyn-Newton method in MATLAB, and benchmark the performance using polynomial test cases. Unless otherwise noted, the experimentations presented in this report are obtained from the test case of a quadratic eigenproblem, where the coefficients $A_0, A_1, A_2$ are all generated using MATLAB's {\tt rand(n)} function with $n=100$. The spectrum of this test problem is illustrated in \ref{fig:E}. 

All calculations were run under MATLAB 8.3 on a Intel Xeon processor with 3.5 GHz and 250GB RAM. 
%--------------------------------------------------------------
\subsection{Benchmarking Newton's Method}
%--------------------------------------------------------------
Figure \ref{fig:NewtonBenchmark} shows the quadratic convergence of Newton's method with respect to the number of iteration. We plot the double log of the relative error for ten samples. The error tolerance is set to $10^{-15}$, which determines the cut-off value for all log-log error plots. The starting points are selected a random point inside the unit circle. 
%--------------------------------------------------------------
\begin{figure}\label{fig:NewtonBenchmark}
\begin{center}
\includegraphics[width=7cm]{../pictures/NewtonConvergence.eps}
\end{center}
\caption{Convergence of Newton method with respect to the number of iterations. \textnormal{Plots represent ten different iterations with random starting points. Linear slope of $log(-log(e_r))$ confirms the quadratic convergence of Newton's method.}}
\end{figure}

\begin{figure}\label{fig:NewtonIterate}
\begin{center}
\includegraphics[width=9cm]{../pictures/Newton_Iteration.eps}
\end{center}
\caption{Trajectory of Newton iteration on the complex plane. \textnormal{The rate of convergence is governed by the quality of the initial guess and the surrounding poles in the vicinity of the updated guess. The transition from stagnant region to quadratic convergence is clearly visible.}} 
\end{figure}
%--------------------------------------------------------------
The relative error represents the normalized difference between adjacent Newton steps as below.
\begin{equation}\label{eq:er}
e_r=|\omega_{j+1} - \omega_j|/|\omega_j|
\end{equation}
The linear slope of the double-log plot $\log(-\log(e_r))$ shows that the convergence of Newton method is indeed quadratic with respect to the number of iterations. The plot also shows the different number of iterations required for each initial guess before the start of actual quadratic convergence. The quality of the initial guess governs fast convergence. Figure \ref{fig:NewtonIterate} illustrates the behavior of $\omega_j$ on the complex plane. Before the start of quadratic convergence, the size of each step is relatively small. After the start each step contributes significantly and arrives at the answer extremely quickly. This shows how a good initial guess can accelerate the initiation of the quadratic convergence and thereby eliminate the idle steps in the beginning. 
%--------------------------------------------------------------
\subsection{Benchmarking Beyn's Method}
%--------------------------------------------------------------
\subsubsection{Convergence of Trapezoidal Sum}
Exponential convergence is expected for the trapezoidal sum of holomorphic periodic integrands. We first check this by plotting the error of two largest eigenvalues with different magnitudes. The error is defined as: 
\begin{equation}
\label{eq:e}
e(\omega_k) = \mathrm{min}\left\lbrace  |\omega_k - \hat{\omega}|:\hat{\omega}\in\sigma_{\mathtt{polyeig}}\right\rbrace ,
\end{equation}
where normalization is omitted for the known answers  $\hat{\omega}_1 = -0.371 + 0.247i$ and $\hat{\omega}_2=
  0.179 + 0.455i$. The result is plotted in figure \ref{fig:Benchmark_Beyn}. 
%--------------------------------------------------------------
\begin{figure}\label{fig:Benchmark_Beyn}
\begin{center}
\includegraphics[width=6.45cm]{../pictures/Benchmark_Beyn.eps}
\includegraphics[width=6.45cm]{../pictures/Benchmark_Beyn_contour.eps}
\end{center}
\caption{Benchmarking Test for Beyn Algorithm. \textnormal{Left: normalized differences $e(\omega_k)$ for $\omega_1$ and $\omega_2$. Right: plot of the exact eigenvalues from $100\times100$ quadratic eigenproblem using {\tt polyeig} (blue stars) and the result of Beyn's method (red circles) with $N=200$, $\hat{M}=rand(100,25)$}}
\end{figure}
%--------------------------------------------------------------
Figure \ref{fig:Benchmark_Beyn} confirms the exponential convergence of Beyn's method using a circular contour around 19 eigenvalues. When both eigenvalues and eigenvectors are concerned, the following residual would be an appropriate benchmark:
\begin{equation}
\label{eq:Residual}
\mathrm{res}=\frac{\| A(\hat{\omega})\mathbf{\hat{v}} \|_2 }{\|A(|\hat{\omega}|) \|_F}.
\end{equation}
%--------------------------------------------------------------

%--------------------------------------------------------------
\subsection{Benchmarking Hybrid Beyn-Newton Method}
%--------------------------------------------------------------
In the hybrid Beyn-Newton method, the results from Beyn's method are used as the initial guess for Newton's iteration until all eigenvalues converge to a desired level ($e_r<10^{-15}$). The quality of the initial guess is determined by the number of quadrature points on the Beyn contour. Six representative plots are shown in \ref{fig:BeynNewton}. 
\begin{figure}\label{fig:BeynNewton}
\begin{center}
\includegraphics[width=6.45cm]{../pictures/NB10.eps}
\includegraphics[width=6.45cm]{../pictures/NB30.eps}
\includegraphics[width=6.45cm]{../pictures/NB50.eps}
\includegraphics[width=6.45cm]{../pictures/NB70.eps}
\includegraphics[width=6.45cm]{../pictures/NB80.eps}
\includegraphics[width=6.45cm]{../pictures/NB150.eps}
\end{center}
\caption{Benchmarking Test for Hybrid Beyn-Newton Method. \textnormal{Accelrated Quadratic Convergence of Hybrid Beyn-Newton Method. Initial guess is given by the output of Beyn's contour integral using $N=10,30,50,70,80,150$. All eigenvalues converge before 5 iterations after the $N>80$.}}
\end{figure}
In figure \ref{fig:BeynNewton}, the convergence of all 19 eigenvalues are plotted together. While most eigenvalues begin with an idle period in $N=10$, quadratic convergence begins right away for 14 eigenvalues already at $N=30$.
As the number of sampling points increases, most eigenvalues converge before reaching the 8th iteration. Only a single pair remains unconverged after 5 iterations when $N=70$. All eigenvalues converge within 5 steps when $N=80$. 

%\begin{figure}\label{fig:Convergence_Hybrid}
%\begin{center}
%\includegraphics[width=6.45cm]{../pictures/jtble.eps}
%\includegraphics[width=6.45cm]{../pictures/Convergence_Newton.eps}
%\end{center}
%\caption{Benchmarking Test for Hybrid Beyn-Newton Method. \textnormal{Left: normalized differences $e(\omega_k)$ for $\omega_1$ and $\omega_2$. Right: plot of the exact eigenvalues from $60 \times 60$ quadratic eigenproblem using {\tt polyeig} (blue stars) and the result of Beyn's method (red circles) with $N=150$, $\hat{M}=rand(60,25)$}}
%\end{figure}

For this particular example, flop count for $N=30$ is smaller than $N=50$ or $N=70$ due to the large computation volume incurred by an increment in $N$. 




A rigorous benchmarking method should automatically determine $N$ and switch to Newton iteration. Further analysis is required to test this for more complicated eigenvalue problems, such as when eigenvalues lie close to the contour or are highly clustered.
%--------------------------------------------------------------
\pagebreak
\subsection{Discussion and Plans}
The purpose of combining the two methods is two-folds. First, compared to Beyn's method alone, the hybrid method requires less computation to arrive at the same level of convergence. Second, compared to Newton's method alone, the use of an educated initial guess guarantees that no eigenvalue will be missed.
\subsubsection{Convergence of Beyn's Method affected by Eigenvalues on the Contour}
Care must be taken to ensure that the contour point does not coincide with an eigenvalue. Figure \ref{fig:overlap} shows how the convergence of Beyn's method is slowed down by the existence of an eigenvalue very close to the contour.
%--------------------------------------------------------------
\begin{figure}\label{fig:overlap}
\begin{center}
\includegraphics[width=9cm]{../pictures/Benchmark_EigenvalueOnContour.eps}\\
\includegraphics[width=6.45cm]{../pictures/Benchmark_EigenvalueOnContour_contour.eps}
\includegraphics[width=6.45cm]{../pictures/Benchmark_EigenvalueOnContour_magn.eps}
\end{center}
\caption{Benchmarking Test for Contour overlapping with an Eigenvalue. \textnormal{Left: normalized differences $e(\omega_k)$ for $\omega_1$ and $\omega_2$. Right: plot of the exact eigenvalues from $60 \times 60$ quadratic eigenproblem using {\tt polyeig} (blue stars) and the result of Beyn's method (red circles) with $N=150$, $\hat{M}=rand(60,25)$}}
\end{figure}

\subsubsection{Independence of convergence from the sampling matrix $\hat{M}$}
We also note that the dimension of the sampling matrix $\hat{M}$ did not produce a clear difference in the eigenvalue convergence. The most important factor for $M\in \mathbb{R}^{n\times l}$ is that it preserves the rank of the original problem, and is not accidentally orthogonal to an existing mode. The use of a random matrix is thus suited for this purpose.

%--------------------------------------------------------------
\section{Conclusion} 
We propose a hybrid solution method for NEPs by using a contour integral method with nonlinear inverse iteration based on Newton's method. We implement and verify that the contour integral method is capable of constructing a successful initial guess for Newton's method. For the test case of 19 eigenvalues inside a circular contour around the origin of the complex plane, the quadrature of N=30 already forced 14 eigenvalues to converge after 8 newton iterations.

The hybridization scheme between the two methods depend on the dimensionality and the nonlinearity involved in each problem. We aim to test more test cases of practical importance using the proposed Beyn-Newton method in the near future. 

%\clearpage
%--------------------------------------------------------------
%\bibliography{Proposalref_20150320}
%\bibliographystyle{plain}
%\bibliographystyle{unsrtnat}
\end{document}